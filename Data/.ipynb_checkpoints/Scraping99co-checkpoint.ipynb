{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set webdriver options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('ignore-certificate-errors')\n",
    "\n",
    "#close any opened driver\n",
    "try:\n",
    "    driver.close() \n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Initiate webdriver\n",
    "driver = webdriver.Chrome(r'C:\\Users\\josep\\Desktop\\Capstone 4\\Data\\chromedriver', options=options)\n",
    "\n",
    "#indicate scraping start page\n",
    "start_page = 50\n",
    "last_page = 228\n",
    "\n",
    "url = f'https://www.99.co/singapore/rent/condos-apartments'\n",
    "driver.get(url+f'?page_num={start_page}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_in_page():\n",
    "\n",
    "    \"\"\"\n",
    "    Get all the house link in for a given page\n",
    "    \"\"\"\n",
    "    links = driver.find_elements_by_xpath(\"//div[@class='_2kH6B']//a[contains(@href,'')]\")\n",
    "    house_links = [link.get_attribute(\"href\") for link in links]\n",
    "    return house_links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WebDriverWait(driver, 60).until(expected_conditions.presence_of_element_located((By.XPATH, '//*[@id=\"appContent\"]/div/div[3]/div[1]/div/div[4]/ul/li[8]/a')))\n",
    "# last_page = driver.find_element_by_xpath('//*[@id=\"appContent\"]/div/div[3]/div[1]/div/div[4]/ul/li[8]/a').text\n",
    "# last_page = int(last_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_one_listing(link):\n",
    "    \n",
    "    listing_detailed = dict()\n",
    "    # WebDriverWait(driver, 60).until(expected_conditions.presence_of_element_located((By.XPATH, '//*[@id=\"price\"]/h2')))\n",
    "    \n",
    "    class_name = '_2sIc2 _29qfj _2rhE-'\n",
    "    WebDriverWait(driver, 60).until(expected_conditions.presence_of_element_located((By.XPATH, f'//*[@class=\"{class_name}\"]')))\n",
    "\n",
    "    \n",
    "    listing_detailed['link'] = link\n",
    "    #main header\n",
    "    try:listing_detailed['price_month'] = driver.find_element_by_xpath('//*[@id=\"price\"]/h2').text\n",
    "    except:listing_detailed['price_month'] = np.nan\n",
    "    \n",
    "\n",
    "    try:listing_detailed['bedroom'] = driver.find_elements_by_xpath(f'//p[@class=\"{class_name}\"]')[0].text\n",
    "    except:listing_detailed['bedroom'] =  np.nan\n",
    "    \n",
    "    try:listing_detailed['bathroom'] = driver.find_elements_by_xpath(f'//p[@class=\"{class_name}\"]')[1].text\n",
    "    except:listing_detailed['bathroom'] =  np.nan\n",
    "    \n",
    "    try:listing_detailed['sqft'] = driver.find_elements_by_xpath(f'//p[@class=\"{class_name}\"]')[2].text\n",
    "    except:listing_detailed['sqft'] =  np.nan\n",
    "\n",
    "\n",
    "    try:listing_detailed['detailed_address'] = driver.find_element_by_xpath('//*[@id=\"overview\"]/div/div[2]/div[1]/p/span[1]').text\n",
    "    except:listing_detailed['detailed_address'] =  np.nan\n",
    "\n",
    "    try:listing_detailed['district'] = driver.find_element_by_xpath('//*[@id=\"overview\"]/div/div[2]/div[1]/p/span[3]').text\n",
    "    except:listing_detailed['district'] =  np.nan\n",
    "\n",
    "    #property details\n",
    "    # This section will take into account these details (if present): Availability, Lease, Furnishing, Property Type, \n",
    "    # Name, Unit Types, Total Units, Built Year, Tenure, Developer, and Neighbourhood\n",
    "    class_name_td1 = '_3r4yN NomDX'\n",
    "    class_name_td2 = '_3r4yN XCAFU'\n",
    "    num_of_property_details = len(driver.find_elements_by_xpath(f\"//div[contains(@class, '{class_name_td1}')]\"))\n",
    "    for i in range(num_of_property_details):\n",
    "        detail_category = driver.find_elements_by_xpath(f\"//div[contains(@class, '{class_name_td1}')]\")[i].text\n",
    "        detail_category = detail_category.lower()\n",
    "        detail_category = detail_category.replace(\" \",\"_\")\n",
    "        listing_detailed[f'{detail_category}'] = driver.find_elements_by_xpath(f\"//div[@class='{class_name_td2}']\")[i].text    \n",
    "    #picture url\n",
    "\n",
    "    try:listing_detailed['picture_url'] = driver.find_element_by_xpath('//*[@id=\"listingPageContent\"]/div[1]/div[1]/div[2]/div/div[1]/img').get_attribute('src')\n",
    "    except:listing_detailed['picture_url'] = np.nan\n",
    "    \n",
    "    #mrt\n",
    "    try:\n",
    "        listing_detailed['mrt_distance'] = driver.find_element_by_xpath('//*[@id=\"listingPageContent\"]/div[1]/div[1]/div[1]/div[2]/p/span').text\n",
    "        listing_detailed['mrt_name'] = driver.find_element_by_xpath('//*[@id=\"listingPageContent\"]/div[1]/div[1]/div[1]/div[2]/p/a').text\n",
    "    except:\n",
    "        listing_detailed['mrt_distance'] = np.nan\n",
    "        listing_detailed['mrt_name'] = np.nan\n",
    "        \n",
    "    #Amenities\n",
    "    # Expand amenities\n",
    "    try:\n",
    "        driver.find_element_by_xpath('//*[@id=\"listingPageContent\"]/div[1]/div[1]/div[3]/button/span').click()\n",
    "    \n",
    "    #wait until button expanded \n",
    "        WebDriverWait(driver, 60).until(expected_conditions.presence_of_element_located((By.XPATH, '//*[@id=\"listingPageContent\"]/div[1]/div[1]/div[3]/button')))\n",
    "    except: pass\n",
    "    #extractall amenities\n",
    "    amenities_elems = driver.find_elements_by_xpath(\"//p[@class='_2sIc2 _2rhE-']\")\n",
    "    try: listing_detailed['amenities'] = [ele.text for ele in amenities_elems]\n",
    "    except: listing_detailed['amenities'] = np.nan\n",
    "    \n",
    "    #scroll down to load map element\n",
    "\n",
    "    # for i in range(1,5):\n",
    "    #     driver.execute_script(f\"window.scrollBy(0,{i*200})\",\"\")\n",
    "    #     time.sleep(0.5)\n",
    "    \n",
    "\n",
    "    # map_element = driver.find_element_by_xpath('//*[@id=\"location\"]')\n",
    "    \n",
    "    # actions = ActionChains(driver)\n",
    "    # actions.move_to_element(map_element).perform()\n",
    "    # time.sleep(1)\n",
    "\n",
    "    #extract distance to key location\n",
    "    # class_name_location_div = '_3OnRG'\n",
    "    # class_name_location = 'yMCxv _1YwzE _1vzK2'\n",
    "    # try: listing_detailed['travel_time_changi'] = driver.find_elements_by_xpath(f\"//div[@class='{class_name_location_div}']/h4[@class='{class_name_location}']\")[0].text\n",
    "    # except: listing_detailed['travel_time_changi'] = np.nan\n",
    "    \n",
    "    # try:listing_detailed['travel_time_raffles'] = driver.find_elements_by_xpath(f\"//div[@class='{class_name_location_div}']/h4[@class='{class_name_location}']\")[1].text\n",
    "    # except: listing_detailed['travel_time_raffles'] = np.nan\n",
    "    \n",
    "    # try:listing_detailed['travel_time_orchard'] = driver.find_elements_by_xpath(f\"//div[@class='{class_name_location_div}']/h4[@class='{class_name_location}']\")[2].text\n",
    "    # except: listing_detailed['travel_time_orchard'] = np.nan\n",
    "    \n",
    "    return listing_detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "try:\n",
    "    scraped_df = pd.read_csv('scraped_df3.csv')\n",
    "    print(\"Existing Data Found\")\n",
    "except:\n",
    "    scraped_df = pd.DataFrame()\n",
    "\n",
    "for _ in range(start_page, last_page+1):\n",
    "    current_page = WebDriverWait(driver, 30).until(expected_conditions.presence_of_element_located((By.XPATH, \"//li[@class='active']\"))).text\n",
    "    current_page = int(current_page)\n",
    "    print(current_page)\n",
    "    if current_page <= last_page:\n",
    "        links = get_link_in_page()\n",
    "        for pg, link in enumerate(links):\n",
    "            driver.get(link)\n",
    "            listing_dict = scrape_one_listing(link)\n",
    "            scraped_df = scraped_df.append(listing_dict, ignore_index=True)\n",
    "            time.sleep(0.5)\n",
    "            print(f\"Completed {pg+1} out of {len(links)} links of Page {current_page}\")\n",
    "        start_page = start_page + 1\n",
    "        print(\"Next Page is \", start_page)\n",
    "        scraped_df.to_csv('scraped_df3.csv', index=False)\n",
    "        driver.get(url+f'?page_num={start_page}')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicate\n",
    "scraped_df = scraped_df.drop_duplicates()\n",
    "scraped_df.to_csv('scraped_df3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d688fef97cee6022be7601baee37833edb40567839e1dc4c10769ccdd01179eb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
